# -*- coding: utf-8 -*-
"""
Xæ¼«ç”»é‡‡é›†æºæµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰æ¥å£çš„æŠ“å–åŠŸèƒ½
"""

import sys
import json
from services.scraper_factory import ScraperFactory

def print_separator(title):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60 + "\n")

def test_categories():
    """æµ‹è¯•è·å–åˆ†ç±»åˆ—è¡¨"""
    print_separator("æµ‹è¯•1: è·å–åˆ†ç±»åˆ—è¡¨")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    print(f"è¯·æ±‚URL: {scraper.base_url}")
    result = scraper.get_categories()
    
    print(f"\nåˆ†ç±»æ€»æ•°: {result['total']}")
    print("\nåˆ†ç±»åˆ—è¡¨:")
    for cat in result['categories'][:10]:
        print(f"  - ID: {cat['id']}")
        print(f"    åç§°: {cat['name']}")
        print(f"    å®Œæ•´URL: {cat['url']}")
        print()
    
    if result['total'] > 10:
        print(f"  ... è¿˜æœ‰ {result['total'] - 10} ä¸ªåˆ†ç±»")
    
    return result['categories'][0]['id'] if result['categories'] else None

def test_category_comics(category_id):
    """æµ‹è¯•è·å–åˆ†ç±»æ¼«ç”»"""
    print_separator(f"æµ‹è¯•2: è·å–åˆ†ç±»æ¼«ç”» (åˆ†ç±»ID: {category_id})")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    url = f'{scraper.base_url}/manga-list-{category_id}-0-10-p1/'
    print(f"è¯·æ±‚URL: {url}")
    result = scraper.get_comics_by_category(category_id, page=1, limit=5)
    
    print(f"\næ¼«ç”»æ•°é‡: {len(result['comics'])}")
    print(f"æ˜¯å¦æœ‰æ›´å¤š: {result['hasMore']}")
    print("\næ¼«ç”»åˆ—è¡¨:")
    
    for comic in result['comics']:
        print(f"  - æ¼«ç”»ID: {comic['id']}")
        print(f"    æ ‡é¢˜: {comic['title']}")
        print(f"    å®Œæ•´å°é¢URL: {comic['cover']}")
        print(f"    æœ€æ–°ç« èŠ‚: {comic.get('latestChapter', 'æ— ')}")
        print(f"    çŠ¶æ€: {comic.get('status', 'æœªçŸ¥')}")
        print()
    
    return result['comics'][0]['id'] if result['comics'] else None

def test_hot_comics():
    """æµ‹è¯•è·å–çƒ­é—¨æ¼«ç”»"""
    print_separator("æµ‹è¯•3: è·å–çƒ­é—¨æ¼«ç”»")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    print(f"è¯·æ±‚URL: {scraper.base_url}/manga-list-31-0-10-p1/")
    result = scraper.get_hot_comics(page=1, limit=3)
    
    print(f"\næ¼«ç”»æ•°é‡: {len(result['comics'])}")
    print("\nçƒ­é—¨æ¼«ç”»:")
    
    for comic in result['comics']:
        print(f"  - æ ‡é¢˜: {comic['title']}")
        print(f"    ID: {comic['id']}")
        print(f"    å°é¢: {comic['cover']}")
        print()
    
    return result['comics'][0]['id'] if result['comics'] else None

def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print_separator("æµ‹è¯•4: æœç´¢æ¼«ç”»")
    
    keyword = "æµ·è´¼ç‹"
    scraper = ScraperFactory.get_scraper('xmanhua')
    search_url = f'{scraper.base_url}/search?keyword={keyword}'
    print(f"æœç´¢URL: {search_url}")
    result = scraper.search_comics(keyword, page=1, limit=3)
    
    print(f"\næœç´¢å…³é”®è¯: {keyword}")
    print(f"ç»“æœæ•°é‡: {len(result['comics'])}")
    print("\næœç´¢ç»“æœ:")
    
    for comic in result['comics']:
        print(f"  - æ ‡é¢˜: {comic['title']}")
        print(f"    ID: {comic['id']}")
        print(f"    å°é¢: {comic['cover']}")
        print()
    
    return result['comics'][0]['id'] if result['comics'] else None

def test_comic_detail(comic_id):
    """æµ‹è¯•è·å–æ¼«ç”»è¯¦æƒ…"""
    print_separator(f"æµ‹è¯•5: è·å–æ¼«ç”»è¯¦æƒ… (ID: {comic_id})")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    detail_url = f'{scraper.base_url}/{comic_id}/'
    print(f"è¯·æ±‚URL: {detail_url}")
    result = scraper.get_comic_detail(comic_id)
    
    if result:
        print(f"\næ ‡é¢˜: {result['title']}")
        print(f"ä½œè€…: {result['author']}")
        print(f"çŠ¶æ€: {result['status']}")
        print(f"åˆ†ç±»: {', '.join(result['categories'])}")
        print(f"è¯„åˆ†: {result.get('rating', 0)}")
        print(f"æ›´æ–°æ—¶é—´: {result.get('updateTime', 'æœªçŸ¥')}")
        print(f"\nå®Œæ•´å°é¢URL: {result['cover']}")
        print(f"\nå®Œæ•´ç®€ä»‹:\n{result['description']}")
    else:
        print("è·å–è¯¦æƒ…å¤±è´¥")
    
    return True

def test_chapters(comic_id):
    """æµ‹è¯•è·å–ç« èŠ‚åˆ—è¡¨"""
    print_separator(f"æµ‹è¯•6: è·å–ç« èŠ‚åˆ—è¡¨ (æ¼«ç”»ID: {comic_id})")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    chapters_url = f'{scraper.base_url}/{comic_id}/'
    print(f"è¯·æ±‚URL: {chapters_url}")
    result = scraper.get_chapters(comic_id)
    
    print(f"\nç« èŠ‚æ€»æ•°: {result['total']}")
    print("\nå‰5ä¸ªç« èŠ‚:")
    
    for chapter in result['chapters'][:5]:
        print(f"  - ç« èŠ‚ID: {chapter['id']}")
        print(f"    æ ‡é¢˜: {chapter['title']}")
        print(f"    é¡ºåº: {chapter['order']}")
        print(f"    æ›´æ–°æ—¶é—´: {chapter.get('updateTime', 'æœªçŸ¥')}")
        print()
    
    if result['total'] > 5:
        print(f"  ... è¿˜æœ‰ {result['total'] - 5} ä¸ªç« èŠ‚")
    
    return result['chapters'][0]['id'] if result['chapters'] else None

def test_chapter_images(chapter_id):
    """æµ‹è¯•è·å–ç« èŠ‚å›¾ç‰‡"""
    print_separator(f"æµ‹è¯•7: è·å–ç« èŠ‚å›¾ç‰‡ (ç« èŠ‚ID: {chapter_id})")
    
    scraper = ScraperFactory.get_scraper('xmanhua')
    image_url = f'{scraper.base_url}/{chapter_id}/'
    print(f"è¯·æ±‚URL: {image_url}")
    
    result = scraper.get_chapter_images(chapter_id)
    
    print(f"\nå›¾ç‰‡æ€»æ•°: {result['total']}")
    print("\næ‰€æœ‰å›¾ç‰‡URL:")
    
    for img in result['images']:
        print(f"  - ç¬¬{img['page']}é¡µ:")
        print(f"    å®Œæ•´URL: {img['url']}")
    
    if result['total'] == 0:
        print("\nâš ï¸  æœªè·å–åˆ°å›¾ç‰‡URLï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é€‰æ‹©å™¨")
    
    return True

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€ å¼€å§‹æµ‹è¯•Xæ¼«ç”»é‡‡é›†æº".center(60, "="))
    
    try:
        # æµ‹è¯•1: è·å–åˆ†ç±»
        category_id = test_categories()
        if not category_id:
            print("âŒ è·å–åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»ID: 31")
            category_id = '31'
        
        # æµ‹è¯•2: è·å–åˆ†ç±»æ¼«ç”»
        comic_id = test_category_comics(category_id)
        if not comic_id:
            print("âŒ è·å–åˆ†ç±»æ¼«ç”»å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return
        
        # æµ‹è¯•3: è·å–çƒ­é—¨æ¼«ç”»
        test_hot_comics()
        
        # æµ‹è¯•4: æœç´¢
        search_comic_id = test_search()
        if search_comic_id:
            comic_id = search_comic_id
        
        # æµ‹è¯•5: è·å–æ¼«ç”»è¯¦æƒ…
        test_comic_detail(comic_id)
        
        # æµ‹è¯•6: è·å–ç« èŠ‚åˆ—è¡¨
        chapter_id = test_chapters(comic_id)
        if not chapter_id:
            print("âŒ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return
        
        # æµ‹è¯•7: è·å–ç« èŠ‚å›¾ç‰‡
        test_chapter_images(chapter_id)
        
        # æµ‹è¯•å®Œæˆ
        print_separator("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("æ‰€æœ‰æ¥å£æµ‹è¯•é€šè¿‡ï¼")
        print("\næç¤º:")
        print("  - å¦‚æœå›¾ç‰‡æ•°é‡ä¸º0ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´CSSé€‰æ‹©å™¨")
        print("  - å¦‚æœæŸäº›æ•°æ®ä¸ºç©ºï¼Œå¯èƒ½æ˜¯CSSé€‰æ‹©å™¨éœ€è¦è°ƒæ•´")
        print("  - å»ºè®®æ£€æŸ¥ç½‘ç«™ç»“æ„æ˜¯å¦æœ‰å˜åŒ–")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
