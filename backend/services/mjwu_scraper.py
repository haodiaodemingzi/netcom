import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin
from .base_video_scraper import BaseVideoScraper


class MjwuScraper(BaseVideoScraper):
    """ç¾å‰§å±‹(mjwu.cc)è§†é¢‘çˆ¬è™«"""
    
    def __init__(self, proxy_config=None):
        super().__init__('https://www.mjwu.cc', proxy_config)
        self.source_id = 'mjwu'
        self.source_name = 'ç¾å‰§å±‹'
        # æ›´æ–°è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.mjwu.cc/',
        })
        self.session.headers.update(self.headers)

    def get_categories(self):
        """è·å–åˆ†ç±»åˆ—è¡¨ - ä»ç½‘ç«™åŠ¨æ€çˆ¬å–"""
        categories = []
        
        try:
            # è¯·æ±‚ç¾å‰§åˆ†ç±»é¡µé¢è·å–æ‰€æœ‰åˆ†ç±»
            url = f'{self.base_url}/type/meiju/'
            response = self._make_request(url)
            if not response:
                return self._get_default_categories()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. æ·»åŠ æ’åºå…¥å£ï¼ˆæœ€æ–°ã€æœ€çƒ­ã€è¯„åˆ†ï¼‰
            categories.append({'id': 'by_time', 'name': 'ğŸ”¥ æœ€æ–°', 'type': 'sort', 'url': '/show/meiju/by/time/'})
            categories.append({'id': 'by_hits', 'name': 'ğŸ“Š æœ€çƒ­', 'type': 'sort', 'url': '/show/meiju/by/hits/'})
            categories.append({'id': 'by_score', 'name': 'â­ é«˜åˆ†', 'type': 'sort', 'url': '/show/meiju/by/score/'})
            
            # 2. è·å–ç±»å‹åˆ†ç±»ï¼ˆå‰§æƒ…ã€å–œå‰§ã€åŠ¨ä½œç­‰ï¼‰
            filter_wraps = soup.find_all('div', class_='hl-filter-wrap')
            
            for wrap in filter_wraps:
                filter_text = wrap.find('div', class_='hl-filter-text')
                if not filter_text:
                    continue
                
                label = filter_text.get_text(strip=True)
                filter_items = wrap.find_all('li', class_='hl-filter-item')
                
                for item in filter_items:
                    link = item.find('a')
                    if not link:
                        continue
                    
                    href = link.get('href', '')
                    name = link.get_text(strip=True)
                    
                    if not name or name == 'å…¨éƒ¨' or not href:
                        continue
                    
                    # è§£æURLç±»å‹
                    if '/class/' in href:
                        cat_type = 'genre'
                        cat_id = f"class_{name}"
                    elif '/area/' in href:
                        cat_type = 'area'
                        cat_id = f"area_{name}"
                    elif '/year/' in href:
                        cat_type = 'year'
                        cat_id = f"year_{name}"
                    else:
                        continue
                    
                    categories.append({
                        'id': cat_id,
                        'name': name,
                        'type': cat_type,
                        'url': href
                    })
            
            # å»é‡
            seen = set()
            unique_categories = []
            for cat in categories:
                if cat['id'] not in seen:
                    seen.add(cat['id'])
                    unique_categories.append(cat)
            
            return unique_categories if unique_categories else self._get_default_categories()
            
        except Exception as e:
            print(f'è·å–åˆ†ç±»å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
            return self._get_default_categories()
    
    def _get_default_categories(self):
        """é»˜è®¤åˆ†ç±»åˆ—è¡¨"""
        return [
            {'id': 'by_time', 'name': 'ğŸ”¥ æœ€æ–°', 'type': 'sort', 'url': '/show/meiju/by/time/'},
            {'id': 'by_hits', 'name': 'ğŸ“Š æœ€çƒ­', 'type': 'sort', 'url': '/show/meiju/by/hits/'},
            {'id': 'by_score', 'name': 'â­ é«˜åˆ†', 'type': 'sort', 'url': '/show/meiju/by/score/'},
            {'id': 'meiju', 'name': 'ç¾å‰§', 'type': 'main'},
            {'id': 'dianying', 'name': 'ç”µå½±', 'type': 'main'},
        ]

    def get_videos_by_category(self, category_id='meiju', page=1, limit=30):
        """æ ¹æ®åˆ†ç±»è·å–è§†é¢‘åˆ—è¡¨"""
        videos = []
        
        try:
            # æ ¹æ®åˆ†ç±»IDæ„å»ºURL
            url = self._build_category_url(category_id, page)
            
            response = self._make_request(url)
            if not response:
                return videos
            
            soup = BeautifulSoup(response.text, 'html.parser')
            videos = self._parse_video_list(soup, limit)
            
        except Exception as e:
            print(f'è·å–è§†é¢‘åˆ—è¡¨å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
        
        return videos
    
    def _build_category_url(self, category_id, page=1):
        """æ ¹æ®åˆ†ç±»IDæ„å»ºURL"""
        # æ’åºåˆ†ç±»ï¼šby_time, by_hits, by_score
        if category_id.startswith('by_'):
            sort_type = category_id.replace('by_', '')
            if page > 1:
                return f'{self.base_url}/show/meiju/by/{sort_type}/page/{page}.html'
            return f'{self.base_url}/show/meiju/by/{sort_type}/'
        
        # ç±»å‹åˆ†ç±»ï¼šclass_å‰§æƒ…
        if category_id.startswith('class_'):
            class_name = category_id.replace('class_', '')
            if page > 1:
                return f'{self.base_url}/show/meiju/class/{quote(class_name)}/page/{page}.html'
            return f'{self.base_url}/show/meiju/class/{quote(class_name)}/'
        
        # åœ°åŒºåˆ†ç±»ï¼šarea_ç¾å›½
        if category_id.startswith('area_'):
            area_name = category_id.replace('area_', '')
            if page > 1:
                return f'{self.base_url}/show/meiju/area/{quote(area_name)}/page/{page}.html'
            return f'{self.base_url}/show/meiju/area/{quote(area_name)}/'
        
        # å¹´ä»½åˆ†ç±»ï¼šyear_2025
        if category_id.startswith('year_'):
            year = category_id.replace('year_', '')
            if page > 1:
                return f'{self.base_url}/show/meiju/year/{year}/page/{page}.html'
            return f'{self.base_url}/show/meiju/year/{year}/'
        
        # é»˜è®¤ä¸»åˆ†ç±»ï¼šmeiju, dianying
        if page > 1:
            return f'{self.base_url}/type/{category_id}/page/{page}.html'
        return f'{self.base_url}/type/{category_id}/'

    def _normalize_cover(self, url):
        """å°†å°é¢åœ°å€è¡¥å…¨ä¸ºå¯è®¿é—®çš„ç»å¯¹åœ°å€"""
        if not url:
            return ''
        # è¿‡æ»¤å ä½æˆ–å†…è”å›¾
        if url.startswith('data:') or 'lightbox-blank' in url or 'placeholder' in url:
            return ''
        if url.startswith('//'):
            return 'https:' + url
        if url.startswith('/'):
            return self.base_url + url
        if not url.startswith('http'):
            return self.base_url + '/' + url
        return url

    def _extract_img_url(self, img):
        """ä»imgæ ‡ç­¾å°½å¯èƒ½å¤šåœ°æå–çœŸå®å°é¢"""
        if not img:
            return ''
        # ç¾å‰§å±‹ä½¿ç”¨æ‡’åŠ è½½ï¼Œå›¾ç‰‡URLåœ¨data-originalå±æ€§ä¸­
        candidates = [
            img.get('data-original', ''),  # ç¾å‰§å±‹ä¸»è¦ä½¿ç”¨è¿™ä¸ª
            img.get('data-src', ''),
            img.get('data-echo', ''),
            img.get('data-lazy', ''),
            img.get('data-cfsrc', ''),
            img.get('src', ''),
        ]
        # å¤„ç† srcset çš„ç¬¬ä¸€ä¸ªåœ°å€
        srcset = img.get('srcset', '')
        if srcset:
            first = srcset.split(',')[0].strip().split(' ')[0]
            candidates.append(first)
        for url in candidates:
            normalized = self._normalize_cover(url)
            if normalized:
                return normalized
        return ''

    def _parse_video_list(self, soup, limit=30):
        """è§£æè§†é¢‘åˆ—è¡¨"""
        videos = []
        seen_ids = set()
        
        # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘é¡¹ - ç¾å‰§å±‹ä½¿ç”¨ hl-list-item æˆ– hl-vod-item ç±»
        video_items = soup.find_all('li', class_=re.compile(r'hl-(list|vod)-item'))
        
        for item in video_items:
            if len(videos) >= limit:
                break
            
            try:
                # æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                link = item.find('a', class_='hl-item-thumb')
                if not link:
                    link = item.find('a', href=re.compile(r'/vod/\d+/'))
                
                if not link:
                    continue
                
                href = link.get('href', '')
                
                # æå–è§†é¢‘ID - ä»URLä¸­æå–
                video_id_match = re.search(r'/vod/(\d+)/', href)
                if not video_id_match:
                    continue
                
                video_id = video_id_match.group(1)
                if video_id in seen_ids:
                    continue
                seen_ids.add(video_id)
                
                # è·å–æ ‡é¢˜ - ä»é“¾æ¥çš„titleå±æ€§æˆ–æ–‡æœ¬ä¸­è·å–
                title = link.get('title', '')
                if not title:
                    title_elem = item.find('div', class_='hl-item-title')
                    if title_elem:
                        title_link = title_elem.find('a')
                        if title_link:
                            title = title_link.get('title', '') or title_link.get_text(strip=True)
                
                if not title:
                    continue
                
                # è·å–å°é¢ - ç¾å‰§å±‹çš„å°é¢URLåœ¨<a>æ ‡ç­¾çš„data-originalå±æ€§ä¸­ï¼Œä¸æ˜¯åœ¨<img>ä¸­
                cover = ''
                # é¦–å…ˆä»aæ ‡ç­¾çš„data-originalè·å–
                cover = link.get('data-original', '')
                if cover:
                    cover = self._normalize_cover(cover)
                
                # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»åµŒå¥—çš„imgæ ‡ç­¾è·å–
                if not cover:
                    img = link.find('img')
                    cover = self._extract_img_url(img)
                
                # è·å–æ›´æ–°çŠ¶æ€ï¼ˆå¦‚ï¼šå…¨8é›†ã€æ›´æ–°è‡³2é›†ç­‰ï¼‰
                status = ''
                status_elem = item.find('span', class_='hl-pic-text')
                if status_elem:
                    status = status_elem.get_text(strip=True)
                
                # è·å–è¯„åˆ†
                score = ''
                score_elem = item.find('span', class_='hl-tag-3')
                if score_elem:
                    score = score_elem.get_text(strip=True)
                
                video = {
                    'id': video_id,
                    'title': title,
                    'cover': cover if cover else f'https://via.placeholder.com/200x300?text={quote(title[:20])}',
                    'source': self.source_id,
                    'status': status,
                    'score': score,
                }
                videos.append(video)
                
            except Exception as e:
                print(f'è§£æè§†é¢‘é¡¹å¤±è´¥: {e}')
                continue
        
        return videos

    def _clean_detail_title(self, title):
        if not title:
            return ''
        safe_title = re.sub(r'\s+', ' ', str(title)).strip()
        safe_title = re.sub(r'\s*[-|ï½œ_]\s*ç¾å‰§å±‹.*$', '', safe_title).strip()
        return safe_title

    def _extract_detail_title(self, soup):
        if not soup:
            return ''

        h1 = soup.find('h1', class_='hl-dc-title')
        if h1:
            title = self._clean_detail_title(h1.get_text(strip=True))
            if title:
                return title

        title_elem = soup.find('div', class_='hl-item-title')
        if title_elem:
            title = self._clean_detail_title(title_elem.get_text(strip=True))
            if title:
                return title

        og_title = soup.find('meta', attrs={'property': 'og:title'})
        if og_title:
            title = self._clean_detail_title(og_title.get('content', ''))
            if title:
                return title

        doc_title = soup.find('title')
        if doc_title:
            title = self._clean_detail_title(doc_title.get_text(strip=True))
            if title:
                return title

        return ''

    def get_video_detail(self, video_id):
        """è·å–è§†é¢‘è¯¦æƒ…"""
        try:
            url = f'{self.base_url}/vod/{video_id}/'
            response = self._make_request(url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–æ ‡é¢˜
            title = self._extract_detail_title(soup)
            
            # è·å–å°é¢ - ç¾å‰§å±‹çš„å°é¢URLåœ¨data-originalå±æ€§ä¸­ï¼Œå¯èƒ½åœ¨<a>æˆ–<span>æ ‡ç­¾ä¸Š
            cover = ''
            # æ–¹æ³•1: ä»span.hl-item-thumbçš„data-originalå±æ€§è·å–
            cover_elem = soup.find('span', class_='hl-item-thumb')
            if cover_elem:
                cover = cover_elem.get('data-original', '')
                if cover:
                    cover = self._normalize_cover(cover)
            
            # æ–¹æ³•2: ä»å°é¢é“¾æ¥a.hl-item-thumbçš„data-originalå±æ€§è·å–
            if not cover:
                cover_link = soup.find('a', class_='hl-item-thumb')
                if cover_link:
                    cover = cover_link.get('data-original', '')
                    if cover:
                        cover = self._normalize_cover(cover)
            
            # æ–¹æ³•3: ä»hl-item-thumb divä¸­è·å–
            if not cover:
                cover_div = soup.find('div', class_='hl-item-thumb')
                if cover_div:
                    # å°è¯•ä»åµŒå¥—çš„å…ƒç´ è·å–
                    for child in cover_div.find_all(['a', 'span', 'img']):
                        cover = child.get('data-original', '') or child.get('src', '')
                        if cover:
                            cover = self._normalize_cover(cover)
                            break
            
            # è·å–è¯„åˆ†
            score = ''
            score_elem = soup.find('span', class_='hl-tag-3')
            if score_elem:
                score = score_elem.get_text(strip=True)
            
            # è·å–æ›´æ–°çŠ¶æ€
            status = ''
            status_elem = soup.find('span', class_='hl-pic-text')
            if status_elem:
                status = status_elem.get_text(strip=True)
            
            # è·å–ç®€ä»‹
            description = ''
            desc_elem = soup.find('div', class_='hl-col-xs-12')
            if desc_elem:
                desc_text = desc_elem.get_text(strip=True)
                # æå–ç®€ä»‹éƒ¨åˆ†
                if 'ç®€ä»‹ï¼š' in desc_text:
                    description = desc_text.split('ç®€ä»‹ï¼š')[-1].strip()
                elif 'å‰§æƒ…ï¼š' in desc_text:
                    description = desc_text.split('å‰§æƒ…ï¼š')[-1].strip()
            
            # è·å–æ¼”å‘˜åˆ—è¡¨
            actors = []
            info_items = soup.find_all('div', class_='hl-full-box')
            for info_box in info_items:
                text = info_box.get_text(strip=True)
                if 'ä¸»æ¼”ï¼š' in text:
                    actor_text = text.split('ä¸»æ¼”ï¼š')[-1].split('å¯¼æ¼”ï¼š')[0].strip()
                    actors = [a.strip() for a in actor_text.split('/') if a.strip()]
                    break
            
            # è·å–æ ‡ç­¾/ç±»å‹
            tags = []
            for info_box in info_items:
                text = info_box.get_text(strip=True)
                if 'ç±»å‹ï¼š' in text:
                    tag_text = text.split('ç±»å‹ï¼š')[-1].split('åœ°åŒºï¼š')[0].strip()
                    tags = [t.strip() for t in tag_text.split('/') if t.strip()]
                    break
            
            # è·å–åœ°åŒº
            area = ''
            for info_box in info_items:
                text = info_box.get_text(strip=True)
                if 'åœ°åŒºï¼š' in text:
                    area = text.split('åœ°åŒºï¼š')[-1].split('å¹´ä»½ï¼š')[0].strip()
                    break
            
            # è·å–å¹´ä»½
            year = ''
            for info_box in info_items:
                text = info_box.get_text(strip=True)
                if 'å¹´ä»½ï¼š' in text:
                    year = text.split('å¹´ä»½ï¼š')[-1].strip()
                    break
            
            detail = {
                'id': video_id,
                'title': title,
                'name': title,
                'cover': cover,
                'score': score,
                'status': status,
                'actors': actors,
                'tags': tags,
                'description': description,
                'area': area,
                'year': year,
                'source': self.source_id,
            }
            
            return detail
            
        except Exception as e:
            print(f'è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
            return None

    def get_episodes(self, video_id):
        """è·å–å‰§é›†åˆ—è¡¨"""
        episodes = []
        
        try:
            url = f'{self.base_url}/vod/{video_id}/'
            response = self._make_request(url)
            if not response:
                return episodes
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ’­æ”¾åˆ—è¡¨ - ç¾å‰§å±‹ä½¿ç”¨ hl-plays-list
            play_list = soup.find('ul', class_='hl-plays-list')
            if not play_list:
                # å°è¯•æŸ¥æ‰¾IDä¸ºhl-plays-listçš„å…ƒç´ 
                play_list = soup.find('ul', id='hl-plays-list')
            
            if play_list:
                # æŸ¥æ‰¾æ‰€æœ‰å‰§é›†é“¾æ¥
                episode_links = play_list.find_all('a', href=re.compile(r'/play/'))
                
                for link in episode_links:
                    try:
                        href = link.get('href', '')
                        if not href:
                            continue
                        
                        # æå–å‰§é›†ä¿¡æ¯ - URLæ ¼å¼ï¼š/play/12561-1-1/
                        episode_match = re.search(r'/play/(\d+)-(\d+)-(\d+)/?', href)
                        if not episode_match:
                            continue
                        
                        series_id = episode_match.group(1)  # è§†é¢‘ID
                        play_source = episode_match.group(2)  # æ’­æ”¾æº
                        episode_num = episode_match.group(3)  # é›†æ•°
                        
                        # æ„é€ å‰§é›†ID
                        episode_id = f"{series_id}_{play_source}_{episode_num}"
                        
                        # è·å–å‰§é›†æ ‡é¢˜
                        episode_title = link.get_text(strip=True)
                        if not episode_title:
                            episode_title = f'ç¬¬{episode_num}é›†'
                        
                        # æ„å»ºæ’­æ”¾URL
                        play_url = urljoin(self.base_url, href)
                        
                        episode = {
                            'id': episode_id,
                            'seriesId': video_id,
                            'title': episode_title,
                            'episodeNumber': int(episode_num),
                            'playUrl': play_url,
                            'source': self.source_id,
                        }
                        episodes.append(episode)
                        
                    except Exception as e:
                        print(f'è§£æå‰§é›†å¤±è´¥: {e}')
                        continue
            
        except Exception as e:
            print(f'è·å–å‰§é›†åˆ—è¡¨å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
        
        return episodes

    def get_episode_detail(self, episode_id):
        """è·å–å•ä¸ªå‰§é›†è¯¦æƒ…ï¼ˆåŒ…å«æ’­æ”¾é“¾æ¥ï¼‰"""
        try:
            import json
            
            # å…ˆè®¿é—®é¦–é¡µè·å–Cookie
            print('è®¿é—®é¦–é¡µè·å–Cookie...')
            home_response = self._make_request(self.base_url)
            if home_response:
                print('æˆåŠŸè·å–é¦–é¡µCookie')
            
            # ä»episode_idä¸­è§£æå‡ºä¿¡æ¯
            # episode_idæ ¼å¼ï¼š12561_1_1
            parts = episode_id.split('_')
            if len(parts) != 3:
                print(f'Invalid episode_id format: {episode_id}')
                return None
            
            series_id, play_source, episode_num = parts
            
            # æ„å»ºæ’­æ”¾é¡µé¢URL - æ ¼å¼: /play/12561-1-1/
            play_page_url = f'{self.base_url}/play/{series_id}-{play_source}-{episode_num}/'
            response = self._make_request(play_page_url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–æ ‡é¢˜
            title = ''
            h2 = soup.find('h2')
            if h2:
                title = h2.get_text(strip=True)
            
            # æŸ¥æ‰¾è§†é¢‘URL - ä»player_aaaaå˜é‡ä¸­æå–
            video_url = None
            
            # ä»é¡µé¢HTMLä¸­æå–player_aaaaå˜é‡
            player_match = re.search(r'player_aaaa\s*=\s*(\{[^<]+\})', response.text)
            if player_match:
                try:
                    player_data = json.loads(player_match.group(1))
                    
                    # è·å–ç¼–ç åçš„URL
                    encoded_url = player_data.get('url', '')
                    
                    if encoded_url:
                        # è¯·æ±‚è§£ææ¥å£è·å–çœŸæ­£çš„m3u8åœ°å€
                        parse_url = f'https://api.apiimg.com/dplay/super.php?id={encoded_url}'
                        
                        # å¸¦ä¸Šcookieå’Œrefererè¯·æ±‚è§£ææ¥å£
                        parse_headers = {
                            'User-Agent': self.headers.get('User-Agent', ''),
                            'Referer': 'https://www.mjwu.cc/',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        }
                        
                        parse_response = self.session.get(parse_url, headers=parse_headers, timeout=15)
                        if parse_response and parse_response.status_code == 200:
                            # ä»è¿”å›çš„HTMLä¸­æå–lineListæ•°ç»„
                            line_match = re.search(r'const\s+lineList\s*=\s*(\[.*?\]);', parse_response.text)
                            if line_match:
                                try:
                                    line_list = json.loads(line_match.group(1))
                                    if line_list and len(line_list) > 0:
                                        # å–ç¬¬ä¸€ä¸ªçº¿è·¯çš„URLä½œä¸ºè§†é¢‘åœ°å€
                                        video_url = line_list[0].get('url', '')
                                        print(f'è·å–åˆ°m3u8åœ°å€: {video_url}')
                                except json.JSONDecodeError as e:
                                    print(f'è§£ælineListå¤±è´¥: {e}')
                        
                        # å¦‚æœæ²¡è·å–åˆ°m3u8ï¼Œä½¿ç”¨è§£æé¡µé¢URLä½œä¸ºå¤‡ç”¨
                        if not video_url:
                            video_url = parse_url
                        
                except json.JSONDecodeError as e:
                    print(f'è§£æplayer_aaaaå¤±è´¥: {e}')
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾iframe
            if not video_url:
                iframe = soup.find('iframe')
                if iframe:
                    video_url = iframe.get('src', '')
            
            episode = {
                'id': episode_id,
                'seriesId': series_id,
                'title': title or f'ç¬¬{episode_num}é›†',
                'episodeNumber': int(episode_num),
                'videoUrl': video_url,
                'playUrl': play_page_url,
                'source': self.source_id,
            }
            
            return episode
            
        except Exception as e:
            print(f'è·å–å‰§é›†è¯¦æƒ…å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
            return None

    def search_videos(self, keyword, page=1, limit=30):
        """æœç´¢è§†é¢‘"""
        videos = []
        
        try:
            # æ„å»ºæœç´¢URL
            # ç¾å‰§å±‹æœç´¢æ ¼å¼ï¼š/search/--/?wd=å…³é”®è¯ æˆ– /search/--/page/{page}.html?wd=å…³é”®è¯
            if page > 1:
                url = f'{self.base_url}/search/--/page/{page}.html?wd={quote(keyword)}'
            else:
                url = f'{self.base_url}/search/--/?wd={quote(keyword)}'
            
            response = self._make_request(url)
            if not response:
                return videos
            
            soup = BeautifulSoup(response.text, 'html.parser')
            videos = self._parse_video_list(soup, limit)
            
        except Exception as e:
            print(f'æœç´¢è§†é¢‘å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
        
        return videos
